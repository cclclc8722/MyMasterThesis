\chapter{文獻回顧}
利用文字探勘方法分析金融市場早已是財務領域重要的研究方向。本章聚焦於財務文本分析之發展，從傳統的字典方法、機器學習至現今所使用的統計計量模型，並探討本研究所使用之統計學習方法 Orthogonal Greedy Algorithm (以下稱 OGA) 以及 Chebyshev Greedy Algorithm (以下稱 CGA)之發展。
\section{財務文本分析}
\subsection{傳統字典方法}
最早應用於財務領域之文字情緒量化模型以建構情緒辭典為主要方法，以 Tetlock (2007)所提出之Harvard-IV辭典為例。該研究從Harvard-IV辭典中選出77個具有相對頻率的負向辭彙並將其分為強、中、弱等類別，實證分析採用<<華爾街日報>>之新聞文章，結果顯示包含負面情緒之新聞文章對於報酬有顯著的影響，發現於發佈新聞當日，道瓊工業平均指數下降了8.1 bps，結果也證實負面消息與正面消息相比較，負面消息影響較大也較全面。

Loughran and McDonald (2011)發現在Tetlock (2007)所提出之辭典中有將近百分之七十五之字詞與財務上常使用之字詞相去甚遠。為了改善Harvard-IV辭典以簡單計數挑選出情緒字詞而量化新聞文章情緒的方式，使用了大量美國10K公司年度財報中的資料，建構出更貼近於金融市場所使用的字詞之辭典，其中包含報酬、交易量等基本財務資訊，使得量化後之情緒分數得以更貼近於真實的情緒。Jeon, McCurdy, and Zhao (2021) 結合 Loughran and McDonald (2011)之辭典並加入了橫截面迴歸與邏吉斯迴歸等方法，發現股票收益跳躍、新聞發佈頻率等二者與內容之相關性顯著，且對於具有較高媒體知名度、分析師覆蓋率和機構所有權的公司，跳躍機率對新聞具有更高的敏感性。

量化文字情緒最簡單快速的方式是為建構情緒辭典，大部分關鍵字詞在財務意義上皆具其合理性與解釋性。然則，建構情緒辭典需依賴大量的主觀選字，在實務上難以消化與解釋大量非結構化資料所產生的特定情緒字詞，也無法因應因時代快速變遷所產生之新詞彙。隨著電腦運算速度提升與機器學習之發展，許多學者研究以機器學習方法量化文本資料之量化模型。
\subsection{機器學習方法}
最早的文字探勘研究主要集中在文本分類和訊息搜索等領域，其中一個重要的成果是樸素貝氏分類器(Naive Bayes Classifier)，Paul Graham (2002) 探討了傳統的垃圾郵件過濾方法的不足之處，並提出了一種基於貝氏定理的過濾方法，並且在實驗中取得了不錯的效果。此分類方法被廣泛應用於文本分類、情感分析等領域，並成為自然語言處理中一個重要的基礎技術。

Gentzkow, Kelly, and Taddy (2019) 透過 Bayesian Regression, Nonlinear Text Regression, Generative Language Models, Word Embeddings等模型進行比較，認為利用機器學習模型可以將文本中有用之訊息轉換為可進行因果分析之量化形式，並將其應用於股票市場的預測、衡量央行演講的情緒與分析媒體內容的政治傾向。

隨著Web的興起，文本資料量急劇增加，文字探勘進入快速發展期，此時出現了新的技術與模型，如主題模型(Topic Model)、情感分析(Sentiment Analysis)和文本摘要(Text Summarization)等。隨著機器學習算法的進步，文字探勘的研究開始轉向基於統計和機器學習的方法，其中一個重要的發展是Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean (2013) 首次提出的 Word2Vec 模型，該模型包括Skip-Gram和CBOW兩種訓練方式，通過大量的文本數據來學習單詞並將單詞轉換為向量表示且具有較高的計算效率，以達到更精確的文本分析。

至於台灣新聞資料的分析中，以 Tien (2019) 使用鉅亨網台灣股市新聞資料進行文本情緒分析為例，其利用了機器學習與深度學習等模型，主要研究文本資料的前處理以及搭配文本分類模型之分類效果的好壞，結果顯示深度學習模型在文字情緒分析上具有較高的準確度與模型穩定性；N-gram字詞特徵提取在三種分類模型中皆能提升準確率，特別是應用在Naive Bayes的分類器上，可以有效地改善需要對模型進行假設的缺點。

機器學習的優勢在於能夠在眾多參數組合的模型中進行搜索，並有效地選擇重要變數與降低模型維度，惟機器學習模型設計複雜較難以解釋且有黑盒子等問題存在，因此有研究使用統計計量模型建構文字量化模型。

\subsection{統計計量模型}
統計計量模型主要貢獻在於建構具有良好統計性質之文字模型，解決了在文字資料常存在之高維度與稀疏性的問題，相較於機器學習模型來說其設計的更簡單且更具有解釋性。

Ke, Kelly, and Xiu (2019)提出Sentiment Extraction via Screening and Topic Modeling (SESTM) 模型，以解決過去情緒辭典模型無法隨著時代演進而更新之動態更新問題；該模型簡單、運算快速且經證明具有良好的統計性質，解決了機器學習模型難以解釋的缺點。實證上使用Dow Jones Newswire研究美國股票市場，該資料集長度、內容完整且具代表性，解決了過去大多文獻只使用少部分資料集測試的問題。Fan, Xue, and Zhou(2021)改進Ke, Kelly, and Xiu (2019)之SESTM模型需對於文字情緒與股價報酬進行過於強烈的假設與監督式學習需要過度依賴過去資料之缺點，提出Factor-Augmented Regularized Model for Prediction (FARM Preict) 模型；其模型以非監督學習的方式從每篇文章中選取重要的字詞做為因子，並分析了中文字詞與英文字詞之間表義與拼音的差異。實證上應用於中國新聞資料庫：<<新浪>>財經新聞，實證結果發現中國市場以散戶投資人為主，相較於正面消息，散戶投資人對於負面的新聞消息具有更大的反應，然而由於政府在放空上有所限制，所以散戶投資人對於負面消息的反應無法反映在投資組合之報酬之中。透過個股新聞資料所計算之情緒分數不僅能確切反應個股報酬且將不受過去股價所影響。


\section{高維度選模方法}
由於非結構化資料的發展與演進，如何有效地處理高維度資料所帶來的問題逐漸浮現，所謂高維度資料，也就是當變數數量超過所觀測到的樣本數時，如果使用標準的迴歸模型估計迴歸係數時，在計算與估計上會有困難(例如:高維度反矩陣不存在)。過往文獻多為透過$L_1$懲罰項之最小平方法($L_1$-Penalized Least Squares)來解決此問題。Tibshirani(1996)首先提出Lasso Regression，並應用於迴歸選模的問題。Lasso主要的使用方法為在目標式中加入係數絕對值總和作為懲罰項(Penalty term)，迫使不相關之係數為0，進而得到一個具有解釋能力的模型。Bü hlmann and Yu (2003)提出$L_2$-boosting，其為基於$L_2$損失函數之梯度下降法所建構之演算法，他們證實在高維度的問題之中，使用立方平滑曲線的$L_2$-boosting是有效且能實踐的。

%Efron, Hastie, Johnstone, and Tibshirani (2004)基於傳統向前選取法(Forward Selection)進行改良，提出更有效之選模方法:Least Angel Regression (LARS)。LARS連結了Lasso與Forward Selection Regression，主要有三大特點:第一，他可以很容易實踐出lasso方法並且花費更少的時間;第二，不同之LARS修改能有效地實踐出向前選取迴歸(Forward Selection Regression);第三，可以很容易的估計出LARS的自由度。
Stepwise Least Square Regression為一個被用以處理大量解釋變數之迴歸分析演算法，由以下三個特性所組成：一，以貪婪(Greedy)的方式進行向前選取，同時將挑選出來的變數與先前挑選的變數進行最小平方迴歸，目的在於最小化殘差平方和(Residual Sum of Squares)；第二，擁有中斷向前選取之規則 (Stopping Rule)；第三，擁有根據一些自訂之準則而向後(Backward selection)刪除所挑選出之變數，以最大化變數與應變數之間的相關性。Ing and Lai (2011)針對高維度迴歸問題，有別於過往使用$L_1$懲罰項之最小平方法，提出在稀疏條件下(Sparsity Condition)基於 Forward Stepwise Component 之高維度迴歸選模演算法 OGA。Ing and Lai (2011) 同時開發出快速迭代過程用以更新 OGA process，因此 OGA 選模與先前所提及之演算法相比擁有更快的運算速度。

本計畫預計將此方法創新並應用於財務文本資料的分析，因 OGA 可以針對超額報酬下挑選重要相關字詞進而建構投資組合。而當報酬為二分類報酬(例如：上漲時設為1，下跌或平盤時設為0)，此時必須使用邏吉斯迴歸模型取代原本於 OGA 模型所使用之線性迴歸模型假設。Temlyakov (2015) 首先提出 CGA 近似方法解決凸形優化問題(Convex Optimization Problem)，也就是說有別以往傳統線性選模問題提升至當損失函數滿足 Convex 特性時有遵循之選模方法。Chen, Dai, Ing, Lai (2019)
則將此CGA方法推廣至高維度下非線性選模演算法(High Dimensional CGA)，因此本計畫透過CGA解決當應變數報酬為二分類報酬時之文本分析問題。
