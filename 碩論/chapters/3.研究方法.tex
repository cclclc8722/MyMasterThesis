\chapter{研究方法}
OGA 模型之優點在於：第一，OGA 不需要過於嚴格的假 設條件，只要應變數為線性假設即可；第二，OGA 透過線性迴歸挑選與應變數相關性較高之解釋變數，因此 OGA 模型具有很好的解釋性以及快速的運算速度。而為了研究當應變數為非線性假設時之選模問題，本研究使用 CGA 模型取代 OGA 模型，CGA 保留了 OGA 的優點並將線性模型推廣至非線性模型，以解決應變數為二分類報酬下之邏吉斯迴歸選模問題。
\section{資料結構}
本研究中收集每篇均有明確標記所對應之個股名稱與代號之新聞文章並建構一個$n$篇新聞文章及$p$個字詞的字詞矩陣，將字詞出現於第$i^{th}$篇新聞文章的次數以字詞計數向量$x_i \in R_+^m$表示。其中 $x_{i,j}$ 則表示字詞$j$在第$i$篇文章中出現的次數$i = 1,2,··· ,n\text{,  }j = 1,2,··· ,p$，此字詞數量向量以矩陣形式(Document-term Matrix)表示如下:
\begin{equation}
X=\begin{pmatrix*}
x_{1,1} & \cdots & x_{1,p} \\
\vdots & \ddots & \vdots \\
x_{n,1} & \cdots & x_{n,p}
\end{pmatrix*}
\end{equation}
{其中$x_{i,j}$ 表示字詞 $j$ 在第 $i^{th}$  篇文章中出現的次數，$y_i$表示第$i^{th}$篇新聞所標記之股票報酬。本研究之研究目的為分析個股新聞情緒是否影響個股報酬，由於模型限制等因素，需移除未標記股票之新聞、標記多於一支股票之新聞、重複新聞與公告類新聞，以維持字詞數量矩陣與目標股票報酬具一對一之關係。由於在一篇文章中每個字詞出現的頻率不盡相同，為了減少出現頻率過少的字詞影響模型估計結果並簡化模型，根據 Fan, Xue, and Zhou (2021) 之建議，透過設定閾值 $\kappa$ ，設定字詞至少出現之次數以減少模型之雜訊(noise)，其公式如下:
}
\begin{equation}
X^{freq}=\{j^{th}~\text{word in}~\bm{X}:k_j \geq \kappa \}
\end{equation}
\section{Pure Greedy Algorithm}
\subsection{迴歸模型設定}
\noindent
考慮線性迴歸模型:
\begin{equation}
\displaystyle y_{i}=\alpha + \sum^{p}_{j=1} \beta_{j}x_{ij}+\varepsilon_{i}, i=1,\ldots,n
\end{equation}
其中$x_{i1},x_{i2},\ldots,x_{ip}$為模型中之$p$個解釋變數，當$p > n$時(i.e. 解釋變數數量大於所觀測到之樣本數量)，使用標準的線性迴歸模型估計迴歸係數時存在計算與統計估計上的困難，然而許多研究證實在稀疏性的條件(Sparsity Condition)成立之下，此時迴歸模型：
\begin{equation}
y(x)=\alpha+\beta^{\top}x, where~\beta=(\beta_{1},\beta_{2},\ldots,\beta_{p})^{\top}, x=(x_{1},x_{2},...,x_{p})^{\top}
\end{equation}
之一致性估計(Consistency Estimation) 仍然有可能存在。

\subsection{PGA process}
在介紹Orthogonal Greedy Algorithm (以下稱OGA)之前，我們需要對Pure Greedy Algorithm (以下稱PGA)選模步驟有初步的認識。PGA為一種逐步迴歸 (Forward Stepwise Regression) 法，由 Bühlmann and Yu (2003) 所提出，PGA選模步驟是透過迭代後生成之殘差，進行分量線性最小平方法而生成迴歸式線性近似值之過程。首先將$y_{i}$與$x_{ij}$分別以$y_{i}-\overline{y}$, $x_{i}-\overline{x}$取代，並假設$\alpha = 0$，使得：
\begin{equation}
\displaystyle y_{i} = \sum_{j=1}^{p} \beta_{j}x_{ij}+\varepsilon_i, i=1,2,\ldots,n 
\end{equation}
，初始化近似值$ \hat{y}_0(x)=0$ (when $k=0$)，並在每次迭代結束後計算殘差:
\begin{equation}
U_i^{(k)}=y_i - \hat{y}_i(x_i),1\leq i \leq n
\end{equation}
，計算完殘差之後挑選 $ x_{i,\hat{j}_{k+1}} $ 與殘差跑迴歸，使得：
\begin{equation}
\displaystyle \hat{j}_{k+1}=argmin_{1\leq j \leq p}\sum_{i=1}^{n}(U_i^{(k)}-\tilde{\beta}_j^{(k)}x_{ij})^2,
\end{equation}
where $\displaystyle \tilde{\beta}_j^{(k)}=\sum_{i=1}^{n} U_i^{(k)x_{ij}} / \sum_{i=1}^{n}x_{ij}^2 $。最後更新近似值：
\begin{equation}
\hat{y}_{k+1}(x)=\hat{y}_{k}(x)+ \tilde{\beta}_j^{(k)}x_{\hat{J}_{k+1}}
\end{equation}
重複上述迭代步驟直到達到預先指定的迭代上限$m$。特別注意，PGA 在每次的迭代過程中，由於沒有針對模型做特定的假設，使得PGA模型是有可能挑選到重複變數的。

\newpage

\section{Orthogonal Greedy Algorithm}
\subsection{OGA process}
Orthogonal Greedy Algorithm (OGA) 模型由 Ing and Lai (2011) 提出，是一種高維度稀疏迴歸模型 (High-dimensional Sparse Regression)之選模方法。

OGA之迴歸模型設定與PGA之迴歸模型設定(式3.3)相同，由於
\begin{equation}
\displaystyle \frac{\sum^{n}_{i=1}(U_i^{(k)}-\tilde{\beta}_j^{(k)}x_{ij})^2}{\sum_{i=1}^{n}(U_i^{(k)})^2}=1-r_j^2
\end{equation}
，其中$r_{j}$為$x_{ij}$與$U_i^{(k)}$之相關係數，因此前述之選模方法等價於在各個迭代過程中挑選與$U_i^{(k)}$相關性最高之解釋變數。而 OGA 的實現是以另一種方式更新近似值，並對向量$ X_{\hat{j}_{k+1}} $進行正交轉換至$ X_{\hat{j}_{k+1}}^{\bot} $，並使用以下更新方式：
\begin{equation}
\hat{y}_{k+1}(x)=\hat{y}_{k}(x)+ \tilde{\beta}_j^{(k)}x_{\hat{J}_{k+1}}^{\bot}, 
\end{equation}
where $\displaystyle \tilde{\beta}_j^{(k)}=\sum_{i=1}^{n} U_i^{(k)}x_{i,\hat{J}_{k+1}}^\bot / \sum_{i=1}^{n}(x_{i,\hat{J}_{k+1}}^\bot)^2$
，藉由依序將解釋變數正交化，如此一來通過分量線性迴歸(Componentwise Linear Regression)來使用最小平方法估計參數，進而避免高維度的反矩陣不存在導致有估計上困難的問題。(Ing and Lai (2011))

由於OGA中挑選$U_i^{(k)}$相關性最強的解釋變數等價於$y_{i}$對$(x_{i,\hat{J}_1},\ldots,x_{i,\hat{J}_k+1})$，然而$U_i^{(k)}$是OGA中$y_{i}$對$(x_{i,\hat{J}_1},\ldots,x_{i,\hat{J}_k+1})$做最小平方迴歸的殘差，代表在選模過程中所選出之$\hat{J}_{k+1}$不會重複被選到(i.e. $j \notin \{\hat{J}_{1},\ldots,\hat{J}_{k+1}\}$)。所以 OGA 與 PGA 選模過程中不同的地方在於，PGA 有可能選到重複的變數而 OGA 則是排除了已經包括在{$\hat{J}_{k+1}$}中所考慮之變數。


\subsection{High Dimensional Information Criterion (HDIC)}
OGA迭代後，迴歸模型中總共有 k 個輸入變數，接下來沿著OGA路徑選擇一個具有最小高維度訊息量準則(HDIC)之模型。令$\hat{\sigma}^{2}_{J} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_{i;J})^{2}$，其中$\hat{y}_{i;J}$代表當$Y$投影到由$X_{J}$所生成的線性空間中$y_{i}$的配適值，定義：
\begin{equation}
HDIC(J) = nlog(\hat{\sigma}^{2}_{J})+\#(J)w_nlog(p)
\end{equation}
\begin{equation}
k_n = argmin_{1\leq k \leq K_n}HDIC(\hat{J}_k)
\end{equation}
其中$\#(\cdot)$代表集合的數量，$\hat{J}_{k}=\{\hat{J}_1,\ldots,\hat{J}_k\}$。而根據不同的對應準則會有不同的$w_{n}$：$HDBIC:w_{n}=logn$適合強稀疏模型，$HDAIC = c, c \geq 2$則適合弱稀疏模型。到此步驟，已經完成 OGA＋HDIC，但此時迴歸模型仍有可能存在不相關之變數，為了完全剔除不相關之變數，需進行模型修剪（Trim）
\subsection{Trim}
定義$\hat{J}_{\hat{k}_n}$之子集合$\hat{N}_{n}$：\\
\begin{center}
$\displaystyle
\hat{N}_{n} = 
\begin{cases}
\{\hat{J}_l:HDIC(\hat{J}_{\hat{k}_n}-\{\hat{J}_l\})>HDIC(\hat{J}_{\hat{k}_n}), 1 \leq l \leq \hat{k}_n\} & \quad \text{if } \hat{k}_n \geq 1 \\
\{\hat{J}_1\} & \quad \text{if } \hat{k}_n = 1
\end{cases}
$
\end{center}
至此我們完成了OGA＋HDIC＋Trim等三個步驟，透過Ing and Lai (2011)理論上之推導可以得到變數選擇之一致性的結果。
\\[2ex]


\begin{algorithm}[h]
  \caption{Orthogonal Greedy Algorithm (Ing et al.(2011))}
  \label{alg::Orthogonal Greedy Algorithm}
  \begin{algorithmic}[1]
  \State $\hat{J_0} \leftarrow \varnothing $ and $ r_0 \leftarrow y $ 
  \For{$i \text{ in } 1:K$}
  \State $\hat{j_i} \leftarrow argmax_{1 \le j \le p}|r^\top_{i-1}X_j|/||X_j||_2 $
  \State $ \hat{J}_i\leftarrow\hat{J}_{i-1}\cup\{\hat{j}_1\} $
  \State $ r_i \leftarrow y - \hat{y}_{\hat{J}_i} $
  \EndFor
  \State $\hat{k} \leftarrow argmin_{1 \leq k \leq K}HDIC(\hat{J}_k)$
  \State $\hat{N} \leftarrow \hat{J}_{\hat{k}}$
  \If {$\#(\hat{N})>1$}
  \For {$l \in \hat{J}_{\hat{k}}$}
  \If {$HDIC(\hat{J}_{\hat{k}}-\{l\}) \leq HDIC(\hat{J}_{\hat{k}})$}
  \State $\hat{N} \leftarrow \hat{N} - \{l\}$
  \EndIf
  \EndFor
  \EndIf \\
  \Return $\hat{N}$
  \end{algorithmic}
\end{algorithm}



\subsection{OGA Predict介紹}
本研究預計將 OGA 選模方法創新，並提出 OGA Predict 方法應用至財務文本的分析之中。以下將針對 OGA predict 各個步驟：篩選常用字詞、挑選重要字詞與計算新聞文章情緒分數進行說明。

由於有些字詞出現頻率過少，為避免在挑選字詞的過程中被這些出現過少之字詞影響模型估計結果，在進入到利用OGA模型挑選重要字詞的步驟之前，參考 Fan, et al. (2021)所提出之建議，第一步利用(3.11)之準則，先挑選出常用字詞以降低模型噪音：
\begin{equation}
X^{freq}=\{j^{th}~\text{word in}~\bm{X}:k_j \geq \kappa \}
\end{equation}
，其中$k_{j}$為$j^{th}$字詞在所有收集到之新聞文章中出現的總次數，$\kappa$為設定為常用字詞之門檻值，$\bm{X}$為所有資料集中字詞之集合。

再來透過 Ing and Lai (2011) 提出之 OGA 步驟挑選與應變數報酬$Y_i$相關性較大之字詞。透過：
\begin{equation}
\displaystyle \hat{j}_{k+1}=argmin_{1\leq j \leq p}\sum_{i=1}^{n}(U_i^{(k)}-\tilde{\beta}_j^{(k)}x_{ij})^2,
\end{equation}
\begin{equation}
\hat{y}_{k+1}(x)=\hat{y}_{k}(x)+ \tilde{\beta}_j^{(k)}x_{\hat{J}_{k+1}}^{\bot}, 
\end{equation}
挑選出與應變數報酬具有相關之字詞，此步驟稱為OGA path。


完成 OGA path 步驟後，利用HDIC以及Trim兩步驟剔除剩餘與報酬不相關之變數，透過以上三個步驟可以成功挑選出與應變數報酬 $Y_i$ 最具相關之字詞$X_{i;\hat{j}_k}$，再利用 OLS 可以得到 $Y_i$ 與 $X_{i;\hat{j}_k}$ 之迴歸估計式:
\begin{equation}
\hat{Y_i} = \hat{\alpha} + \hat{\beta}^{\bot}X_{i;\hat{j}_k}
\end{equation}
最後，將所要估計之新文章之$X_{new;\hat{j}_k}$計算出後，將其帶入(式 3.14)，即可得到新文章之情緒分數：
\begin{equation}
Y_{new} = \hat{\alpha} + \hat{\beta}^{\bot}X_{new;\hat{j}_k}
\end{equation}

\newpage

\section{Chebyshev Greedy Algorithm}
由於 OGA 模型在迴歸模型設定假設為一般線性迴歸模型，因此當應變數設定為非線性假設時，必須修正先前之模型假設。Chen, Dai, Ing, Lai 設計高維度非線性選模演算法 CGA process，當迴歸模型為convex function時皆適用此高維度選模方法CGA predict，並解決當應變數為非線性假設下之選模問題。

CGA是利用對數概似函數對參數偏微分之大小作為選模依據，挑選出偏微分後最大之變數，以下依序介紹 CGA 選模以及 CGA Predict 模型。
\subsection{迴歸模型設定}
考慮邏吉斯迴歸模型：
\begin{equation}
E(y_t) = \displaystyle \frac{e^{\alpha+\sum_{j=1}^{p}\beta_jx_{ij}}}{1+e^{\alpha+\sum_{j=1}^{p}\beta_jx_{ij}}}, i = 1,2,\ldots,n
\end{equation}
不同於OGA針對高相關性進行選模，CGA是透過對數概似函數對參數偏微分之大小作為選模依據，因此計算邏吉斯迴歸之對數概似函數：
\begin{equation}
\ell = \sum_{i}[y_i(\alpha + \sum_{j=1}^{p}\beta_jx_{ij}) - log(1 + exp(\alpha+\sum_{j=1}^{p}\beta_jx_{ij}))]
\end{equation}

\subsection{CGA process}
在CGA迭代過程中，首先將解釋變數進行標準化並且初始化$\beta_{i}=0$，並利用最大概似估計法(maximum likelihood function, MLE)估計每一次迭代選到之參數並將估計值代回下式，則第k + 1 次選模準則為：
\begin{equation}
\hat{J}_{k+1} = argmax_{1 \leq j \leq p}|\nabla_j\ell(\hat{\beta}_{\hat{J}_{k+1}})|
\end{equation}
由於 CGA 不像 OGA 有正交特性，所以如果挑選到重複的變數時，則依序挑選偏微分值第二大之變數。第二步將第 k+1 次選到之變數聯集前 k 次挑選出變數 $(\hat{J}_{k+1} = \hat{J}_{k} \cup \hat{J}_{k+1})$，透過 MLE 得參數估計為：
\begin{equation}
\hat{\beta}_{\hat{J}_{k+1}} = argmax_{\beta}(\beta)
\end{equation}


\subsection{High Dimensional Information Criterion (HDIC)}
CGA迭代後，迴歸模型中總共有 k 個輸入變數，接下來沿著CGA路徑選擇一個具有最小高維度訊息量準則(HDIC)之模型。
\begin{equation}
HDIC(J) = -\ell(\hat{\beta}_J)+\#(J)w_n(log(p/n))^{1/2}
\end{equation}
\begin{equation}
k_n = argmin_{1\leq k \leq K_n}HDIC(\hat{J}_k)
\end{equation}
其中$\#(\cdot)$代表集合的數量，$\hat{J}_{k}=\{\hat{J}_1,\ldots,\hat{J}_k\}$。而根據不同的對應準則會有不同的$w_{n}$：$HDBIC:w_{n}=logn$適合強稀疏模型，$HDAIC = c, c \geq 2$則適合弱稀疏模型。到此步驟，已經完成 CGA＋HDIC，但此時迴歸模型仍有可能存在不相關之變數，為了完全剔除不相關之變數，需進行模型修剪（Trim）
\subsection{Trim}
定義$\hat{J}_{\hat{k}_n}$之子集合$\hat{N}_{n}$：\\
\begin{center}
$\displaystyle
\hat{N}_{n} = 
\begin{cases}
\{\hat{J}_\ell:HDIC(\hat{J}_{\hat{k}_n}-\{\hat{J}_\ell\})>HDIC(\hat{J}_{\hat{k}_n}), 1 \leq \ell \leq \hat{k}_n\} & \quad \text{if } \hat{k}_n \geq 1 \\
\{\hat{J}_1\} & \quad \text{if } \hat{k}_n = 1
\end{cases}
$
\end{center}
至此我們完成了CGA＋HDIC＋Trim等三個步驟。
\\[2ex]

\begin{algorithm}[h]
  \caption{Chebyshev Greedy Algorithm (Chen et al.)}
  \label{alg::conjugateGradient}
  \begin{algorithmic}[1]
  \State $\hat{J}_0 \leftarrow \varnothing $, $ \hat{J}_T \leftarrow \varnothing $, $ \hat{\beta}_{\hat{J}_0} \leftarrow 0 $
  \For{$m \text{ in } 1:K_{n}$}
  \State $\hat{j}_m \leftarrow argmax_{1 \le j \le p_n}|\bigtriangledown_j\ell_n(\hat{\beta}_{\hat{j}_{m-1}})| $
  \State $ \hat{J}_m\leftarrow\hat{J}_{m-1}\cup\{j_m\} $
  \State $ \hat{\beta}_{\hat{j}_{m-1}} \leftarrow argmin_{\beta:\beta(\hat{J}^c_{m-1})=0}\ell_{n}(\beta) $
  \EndFor
  \State $\hat{k} \leftarrow argmin_{1 \leq k \leq K}HDIC(\hat{J}_k)$
  \For {$k \text{ in } 1:\hat{k}_{n}$}
  \If {$HDIC(\hat{J}_{\hat{k}_n} - \{j_k\}) < HDIC(\hat{J}_{\hat{k}_n})$}
  \State $\hat{J}_{T} \leftarrow \hat{J}_{T} \cup \{j_k\}$
  \EndIf
  \EndFor
  \State $\hat{\beta} \leftarrow argmin_{\beta:\beta(\hat{J}^c_{T} = 0)}\ell_{n}(\beta)$\\
  \Return $\hat{\beta}$
  \end{algorithmic}
\end{algorithm}

\subsection{CGA Predict介紹}
與OGA predict之選模步驟相同，第一步會先去除出現頻率過低之字詞以降低這些字詞影響模型估計之準確性，一樣參考Fan, et al. (2021)之建議，利用以下準則篩選掉出現頻率過低之字詞：
\begin{equation}
X^{freq}=\{j^{th}~\text{word in}~\bm{X}:k_j \geq \kappa \}
\end{equation}
，其中$k_{j}$為$j^{th}$字詞在所有收集到之新聞文章中出現的總次數，$\kappa$為設定為常用字詞之門檻值，$\bm{x}$為所有資料集中字詞之集合。

第二階段透過 Chen, et al. (2019) 提出的 CGA 步驟挑選出對數概似函數對參數偏微分最大之字詞。選模之第一步驟稱為 CGA path，透過(式 3.18):
\begin{center}
$\hat{J}_{k+1} = argmax_{1 \leq j \leq p}|\nabla_j \ell(\hat{\beta}_{\hat{J}_k+1})|$
\end{center}
挑選出概似函數對變數偏微分最大之字詞。

完成 CGA path 步驟後，利用HDIC以及Trim兩步驟剔除剩餘與報酬不相關之解釋變數，成功挑選出對數概似函數對變數偏微分最大之字詞 $X_{i;\hat{j}_k}$，再來利用最大概似估計法可以得到 $Y_i$ 與 $X_{i;\hat{j}_k}$ 邏吉斯迴歸估計式:
\begin{equation}
\hat{Y_i} = \displaystyle \frac{e^{\hat{\alpha}+\sum_{j=1}^{p}\hat{\beta}_jX_{i;\hat{j}_k}}}{1+e^{\hat{\alpha}+\sum_{j=1}^{p}\hat{\beta}_jX_{i;\hat{j}_k}}}
\end{equation}
最後，將所要估計之新文章之$X_{new;\hat{j}_k}$計算出後，將其帶入(式 3.14)，即可得到新文章之情緒分數：
\begin{equation}
Y_{new} = \displaystyle \frac{e^{\hat{\alpha}+\sum_{j=1}^{p}\hat{\beta}_jX_{new;\hat{j}_k}}}{1+e^{\hat{\alpha}+\sum_{j=1}^{p}\hat{\beta}_jX_{new;\hat{j}_k}}}
\end{equation}


%%如上只需使用\input{表格路徑}即可插入表格。所插入的表格會自動依照章節編入表目錄中，無需再進行更動。
%%若需在文章中引用表格可採用~\ref{表格章節編號}。